{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#importing needed libraries and defining symbolic function 'x' using 'symbols' function\n",
        "import sympy as sp\n",
        "x = sp.symbols('x')\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plv4P8uWIJuZ",
        "outputId": "c8fcde19-8f25-4a2b-82a7-1f114b958e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the list of functions for which we will generate taylor expansions up to 4th degree\n",
        "functions = [sp.sin(x), sp.cos(x), sp.exp(x), sp.log(x + 1)]\n",
        "\n",
        "print(\"Original Functions:\")\n",
        "for function in functions:\n",
        "    print(function)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tqbhvpiIJxQ",
        "outputId": "f4d806ff-9b22-43bd-ff0f-4f3130a0577c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Functions:\n",
            "sin(x)\n",
            "cos(x)\n",
            "exp(x)\n",
            "log(x + 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the point at which we want to henerate taylor expansions and list to store the tokenized data\n",
        "point = 0\n",
        "data = []\n"
      ],
      "metadata": {
        "id": "A_nnaCgoIJ0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loop over the list of functions and generate their tayler expansion up to the 4th order.\n",
        "for function in functions:\n",
        "    taylor_expansion = function.series(x, point, n=5).removeO()\n",
        "    data.append((str(function), str(taylor_expansion)))\n",
        "\n",
        "print(\"\\nTaylor Expansions:\")\n",
        "for _, taylor_expansion in data:\n",
        "    print(taylor_expansion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46tEbDjXIJ27",
        "outputId": "fe6af7a9-21b0-4fcb-f538-30b742ba4226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Taylor Expansions:\n",
            "-x**3/6 + x\n",
            "x**4/24 - x**2/2 + 1\n",
            "x**4/24 + x**3/6 + x**2/2 + x + 1\n",
            "-x**4/4 + x**3/3 - x**2/2 + x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing the data using 'tokenize' function from 'nltk' library\n",
        "import nltk\n",
        "tokenizer = nltk.RegexpTokenizer('\\w+')\n",
        "tokenized_data = [(tokenizer.tokenize(function), tokenizer.tokenize(taylor_expansion)) for function, taylor_expansion in data]\n",
        "print(\"\\nTokenized Data:\")\n",
        "for function, taylor_expansion in tokenized_data:\n",
        "    print(function, \"->\", taylor_expansion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55q6uD2gIJ5f",
        "outputId": "c7b5e805-178e-4474-ef30-029e049391d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized Data:\n",
            "['sin', 'x'] -> ['x', '3', '6', 'x']\n",
            "['cos', 'x'] -> ['x', '4', '24', 'x', '2', '2', '1']\n",
            "['exp', 'x'] -> ['x', '4', '24', 'x', '3', '6', 'x', '2', '2', 'x', '1']\n",
            "['log', 'x', '1'] -> ['x', '4', '4', 'x', '3', '3', 'x', '2', '2', 'x']\n"
          ]
        }
      ]
    }
  ]
}